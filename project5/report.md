# <center> 中山大学计算机学院人工智能本科生神经网络实验报告
> <center>（2022学年春季学期）
------
<font size=2>课程名称：Artificial Intelligence</font>

<div class="center">
## 一.实验题目
**使用神经网络在给定文本数据集完成文本情感分类训练，在测试集完成测试， 计算准确率。**


## 二.实验内容

### 1.算法原理：

- ### 神经网络基础：

  - **结构**：一个典型的神经元一般包含有3个输入，1个输出，以及2个计算功能。输入、输出与计算功能间的链接一般含有权重。
  - **单层神经网络(感知器MP模型)：**有输入层和输出层两个层次。输入层里的“输入单元”只负责传输数据，不做计算；输出层里的“输出单元”则需要对前面一层的输入进行计算。感知器类似一个**逻辑回归**模型，可以做线性分类任务。一般使用sgn作为激活函数。
  - **两层(多层)神经网络(多层感知器)：**新增加了一个或多个中间层，与输出层同为计算层。除了每层的节点外，还有默认存在的偏置节点，其存储值永远为1，仅含有存储功能。**一般使用sigmoid函数作为激活函数g。**
  - **损失：**使用神经网络预测的结果大部分都与实际值有所偏差，这偏差值成为损失，将神经网络的预测值与之结合可以得出**损失函数**，使损失函数值尽量小是我们的目标。

- ### 梯度下降算法(Gradient Descent):

  - 使损失函数值尽量小是一个优化问题，为了处理该问题我们将使用梯度下降算法。

  - **梯度**：梯度是一个向量，表示某一函数在该点出的方向导数 沿着该方向取得最大值。

  - 每次计算参数在当前的梯度，然后让参数向着梯度的反方向前进一段距离，不断重复，直到梯度接近零时截止。一般这个时候，所有的参数恰好达到使损失函数达到一个最低值的状态。

  - 本次实验我们将以
    $$
    权重 = 权重 - 学习率×∂损失函数/∂权重
    $$
    形式使用该算法。

- ### BP算法(**Error Back Propagation**):

  - 神经网络中，因为权重W与其他许多变量都有关联，结构复杂，所以每次计算所有的梯度的代价非常之大，需要与**BP算法(反向传递)**配合。
  - BP算法由两部分组成，向前传播和向后传播：向前传播从输入层经过隐藏层到输出层，从而给出预测结果。在第一次传输过程中，使用到的权重是随机初始化的，因此第一次的预测结果会与真实值差很多。
  - 向后传播：基于真实值与预测值间的误差，以**梯度下降法**为核心，**求每个权重对应误差的偏导数值，以此作为修正权重的依据。**

### 2.相关思路以及流程图：
- ### **相关思路** ：

    #### 文本情感分类问题：

    - 给一定数量的文本作为学习样本，作为输入进入神经网络，每条文本拥有相关对应的情感，句子中出现的每一个单词作为输入的节点之一，经过学习，更新权重w使预测函数值拟合真实值。之后输入测试用文本，得到预测结果。

    #### BP算法实现：

    1. 读入学习用文本，按句读取，将每句文本中出现的每一个单词作为一个输入节点。
    1. 使用学习用文本逐句进行学习，初始化随机权重w1和w2计算出**隐藏层计算结果和输出层计算结果，由此得到对应预测的文本情感。**
    1. 之后按照**反向传递算法**，将实际文本情感与预测文本情感带入**损失函数**得到损失函数值，**求出隐藏层与输出层的损失函数值。**
    1. 使用**梯度下降算法，得到新的权重w1、w2**。
    1. 循环2、3、4直到学习文本学习完毕。
    1. **使用学习过的权重w1、w2与测试案例构成新的网络进行预测。**
    
    #### 损失算法代码实现理论
    
    - ##### 损失函数选择均方误差(MSE)。
    
    - 首先取得损失函数值，正常来说我们直接将**预测值与真实值相减**得到误差，但对网络中所有节点误差求和，会发现其值为0，这意味着理论上没有误差产生，但这是不可能的。所以我们采用**预测值与真实值之差的平方**作为误差。则误差函数关于权重的偏导为：
    
      <img src = 'img/loss1.png'>
    
      - **E为误差，t为实际值，o为预测值(输出)**
    
      **又因为节点输出仅取决于与节点链接的权重w，所以求和变得不再必要**
    
      <img src = 'img/loss2.png'>
    
      而我们的输入可以通过sigmoid函数求的，带入上式并化简可以得到
    
      <img src = 'img/loss3.png'>
    
    - 由上述提及**梯度下降算法**所用公式
      $$
      wjk = wjk - η×∂E/∂wjk
      $$
    
    #### 流程图：
    
    <img src = 'img/mindmap.png'>
    
    
### 3.关键代码展示：

#### 一、预设(记录文本所有可能出现单词)：

```python
total_word = {}	#所有出现的单词
train_sentence = []	#训练文本出现语句
def get_train() :	#获取所有出现单词
    with open(train_set) as f:
        for sentence in f.readlines():	#按行读文件   
            temp = sentence.split(" ")	
            train_sentence.append(temp)	#存入训练文本语句
            for i in range(3 , len(temp)):	#将文本单词存入total_word
                if(temp[i] not in total_word):#新单词为其设置新的标记
                    total_word[temp[i]] = len(total_word)
```

- 程序运行首先输入城市节点数，然后按照顺序形成矩阵。之后通过计算欧氏距离得到邻接矩阵。

#### 二、所需函数准备：

- #### **Sigmoid函数**

  ```python
  def sigmoid(out):
      return 1 / (1 + np.exp(-out))
  ```

  - 公式如下
    $$
    S(x) = 1 / 1 + e的-x次方
    $$

  - 图像如下

    <img src = 'img/sigmoid.png'>

- 

- #### **将两个节点中间所有节点插入第三个节点后**

  ```python
  def get_emotion(pin) :#获取文本情感对应的矩阵
      res = np.zeros((6,1))
      if(pin == "anger") :
          res[0,0] = 1
      elif(pin == "disgust") :
          res[1,0] = 1
      elif(pin == "fear") :
          res[2,0] = 1
      elif(pin == "joy") :
          res[3,0] = 1
      elif(pin == "sad") :
          res[4,0] = 1
      elif(pin == "surprise") :
          res[5,0] = 1
      return res
  ```

  - 矩阵要对齐，所以**文本的真实情感转化的矩阵也为[6:1]。**

#### 三、训练函数(BP算法)：

```python
def train(w1,w2,total_word,learning_rate) :
    current_right = 0	#预测情感正确数
    for i in range(len(train_sentence)) :	#对学习文本逐句进行训练
        inp = np.zeros((len(total_word),1))
        for w in train_sentence[i][3:] :
            inp[total_word[w],0] = 1
        hide = np.dot(w1,inp)	#根据输入得到隐藏层
        out = sigmoid(np.dot(w2,hide))	#根据隐藏层得到输出
        out_loss = out - get_emotion(train_sentence[i][2])#损失函数
        out_delta = out_loss*out*(1-out)#求偏导
        
        hide_loss = np.dot(w2.T, out_loss)    #损失函数
        hide_delta = hide_loss   #求偏导
        
        w2 -= np.dot(out_delta, hide.T)*learning_rate   #w = w - Learnin g_rate * dw
        w1 -= np.dot(hide_delta, inp.T)*learning_rate
        for j in range(6) :
            if jugde_which_emotion(out)[j,0] == 1 :
                break
        if emotion[j] == train_sentence[i][2] :
            current_right += 1 
    return current_right,i+1
```

- 就如上述流程图中的步骤进行。
  - 第一个for循环为了逐句分析学习文本
  - 第二个for循环是将**当前句子出现的单词标记为“存在”(1),**由此生成输入矩阵。
  - 接下来便是按照(第一次为**随机**取得)w1、w2得出hide和out。
  - 之后按照反向传递得到新的权重w1，w2

- 其中权重更改的代码实现均与先前提及理论对应：

  <img src = 'img/loss3.png'>

  - out_loss对应上述损失值 - (tk-ok)
  - sigmoid(Σjwjk×oj)对应**开始求的out**
  - 而out_delta就对应**sigmoid(Σjwjk×oj)×(1-sigmoid(Σjwjk×oj))**
  - hide隐藏层相关与out几乎一致，便不再赘述。

- **其中反向传递以及梯度下降的实现与相关思路中一致，便不再赘述。**

#### 四、测试用函数及数据：

```python
def test(w1,w2,total_word) : 
    right_num = 0	#预测正确数
    line = 0	#测试文本句数
    with open(test_set) as f0 :
        for sent in f0.readlines() :
            line += 1	#按行读入预测文本
            inp = np.zeros((len(total_word),1))
            words = sent.split(' ')
            for i in words[3:] :	#获取输入层
                if i in total_word :
                    inp[total_word[i],0] = 1
            hide = sigmoid(np.dot(w1,inp))	#按照训练得到权重计算
            out = sigmoid(np.dot(w2,hide))
            for i in range(6) :	#判断表达情感
                if jugde_which_emotion(out)[i,0] == 1 :
                    break
            if emotion[i] == words[2] :	#判断是否预测成功
                right_num += 1
    print('correct : '+ str(right_num)+' rate : ' + str(right_num/line))
```

- **生成输入层的部分与学习函数时一致。**
- 之后便是将获取的输入层带入网络即可

```python
learning_rate = 0.1	#学习率为0.1
rd = np.random.RandomState(7) #300-79 303-63
w1 = rd.uniform(0, len(total_word)**-0.5, (100,len(total_word)))#使用均匀分布取随机数
w2 = rd.uniform(0, 100**-0.5, (6,100))
 for i in range(12) :	#对学习样本进行多次学习
        right_num,line = train( w1, w2, total_word, learning_rate)
        i+=1
```

- 均匀分布种子设为7

### 4.代码改动：
- #### 权重w初始值的选取

  因为权重w的初始值会影响学习的收敛速度和方向，所以选择合适的w获取可以使准确率提高：

  - 更改w选取的随机种子：
  - 将w选取的均匀分布改为**高斯分布**：

  ```python
  w1 = np.random.normal(0, len(total_word)**-0.5, (100,len(total_word)))
  w2 = np.random.normal(0, 100**-0.5, (6,100))
  ```

  **上述更改的结果将在实验结果中进行对比分析。**

- #### **对神经网络输出结果标准化**

  在标准化前，out作为[6:1]的矩阵，**每个元素的和大于1，在初始化时为3 。**这导致在计算损失值时会出现大量负数，且不符合实际。因此使用以下函数，使out中元素的和为1，更符合实际，从而提高准确率。

  ```python
  def get_stand(out) :    #out数组标准化
      sum = np.sum(out)
      for i in range(6) :
          out[i][0] = out[i][0]/sum#将out中的值按比例分配
      return out 
  ```

  对应out和损失函数求值为

  ```python
  out = get_stand(sigmoid(np.dot(w2,hide)))	#根据隐藏层得到输出
  out_loss = out - get_emotion(train_sentence[i][2])
  out_delta = out_loss*(1-out)*out
  ```

​		**上述更改的结果将在实验结果中进行对比分析。**

- #### 损失函数计算时是否将输出归一化

  同上述标准化的理由，out作为[6:1]的矩阵，每个元素的和大于1，在初始化时为3 。因此欲设置函数，在每次计算出out后，选出out**矩阵中数值最大的部分，置1，其他置0，这也可以使元素和为0**

  ```python
  def jugde_which_emotion(out) :#判断文本情感
          res = np.zeros((6,1))
          max = out.max()
          for i in range(6) :	#取输出矩阵中最大值所对应的情感作为文本情感
              if(out[i,0]==max) :
                  res[i,0] = 1
          return res
  ```

  - 因为总共有6种情感，**所以用一个矩阵[6:1]存储判断情感**
  - 根据输出层out[6:1]的值，取输出矩阵中最大值所对应的情感作为文本情感。

  其out和对应损失函数求值为：

  ```python
  out = sigmoid(np.dot(w2,hide))
  out_loss = jugde_which_emotion(out)-get_emotion(train_sentence[i][2])
  out_delta = out_loss*jugde_which_emotion(out)*(1-jugde_which_emotion(out))
  ```

  事实上归一化的改进并不能提升准确率，对比结果如下。

## 三.实验结果及分析
#### 实验结果

- ##### 运行结果：

  学习前结果：

  <img src="img/r1.png">

    - ##### correct代表预测正确条目，rate代表预测正确概率

  学习后结果:

  <img src="img/r2.png">

  会额外将判断结果保存在result.txt文件中:

  <img src="img/r3.png">

- ##### 实验使用到数据集中test.txt和train.txt两个文件，其中test.txt有1001条语句，train.txt有246条语句，所以有两种情况

  - #### 使用test来学习，train来测试

  - #### 使用train来学习，test来测试

- ##### 接下来将对各种变量以及学习样本进行实验结果的分析。

#### 分析

 **以下按照使用test来学习，train来测试的数据形式对结果进行对比分析**

- 此处为代码改动中的**归一化和标准化对比**采用均匀分布、学习率0.1、epoch上限为12：

  |  更改样式  | 正确数 |       正确率        |
  | :--------: | :----: | :-----------------: |
  |   未改动   |   82   | 0.3319838056680162  |
  | **标准化** |   88   | 0.3562753036437247  |
  |   归一化   |   48   | 0.19433198380566802 |

  - 由此可见，归一化并没有提升准确率反而降低，而标准化能，**因此采用标准化。**

- ##### 权重w的初始值取值方式取均匀分布，学习率设为0.1，epoch上限为12，对随机种子进行探究

  | 种子  | 正确数 |       正确率       |
  | :---: | :----: | :----------------: |
  |   5   |   86   | 0.3481781376518219 |
  |   6   |   85   | 0.3441295546558704 |
  | **7** |   88   | 0.3562753036437247 |
  |   8   |   88   | 0.3562753036437247 |
  |   9   |   87   | 0.3522267206477733 |

  - 由上述实验结果可以看出**种子取7最合适。**

- ##### 探究权重w的初始值取值方式，均匀分布种子取7，学习率设为0.1，epoch上限为10

  | 概率函数 |       正确数       |              正确率               |
  | :------: | :----------------: | :-------------------------------: |
  | 均匀分布 | 88\|88\|88\|88\|88 | 0.356\|0.356\|0.356\|0.356\|0.356 |
  | 高斯分布 | 89\|89\|84\|85\|89 | 0.360\|0.360\|0.340\|0.344\|0.360 |

  - 因为均匀分布在设置种子后，取得的伪随机数就不会再改变，因此每次获取权重几乎没差别，而**正态分布是往往有一个极值，因此实验结果浮动较大.**
  - 由上述实验结果可知选择正态分布有概率取得更高的准确率，选择均匀分布会得到更稳定的结果。

- ##### 对学习率进行探究，epoch上限取12，权重初值以均匀分布取值

  |  学习率  | 正确数 |       正确率        |
  | :------: | :----: | :-----------------: |
  |   0.01   |   79   | 0.31983805668016196 |
  |   0.05   |   79   | 0.31983805668016196 |
  |   0.1    |   88   | 0.3562753036437247  |
  | **0.15** |   90   | 0.3643724696356275  |
  |   0.2    |   84   |  0.340080971659919  |
  |   0.3    |   43   | 0.17408906882591094 |
  |   0.4    |   49   | 0.19838056680161945 |

  - 由上述实验结果可以得出学习率取0.15最佳。

#### 意外发现

在测试时发现，将损失函数中：**sigmoid(Σjwjk×oj)×(1-sigmoid(Σjwjk×oj))**删去**sigmoid(Σjwjk×oj)**，即：

```python
out_loss = out - get_emotion(train_sentence[i][2])
out_delta = out_loss*(1-out)
```

时，这时候采用**学习率0.1，epoch上限取12，权重初值以均匀分布取值**可以达到更高的准确率**0.381**：

<img src="img/r4.png">

而**学习率为0.15时**，准确率略低，但是还是高过先前的实验结果:

<img src="img/r5.png">

- #### 之后更换各种参数都无法超过94条语句正确的记录，其他参数下的正确数均在88~90之间，便不再额外占据篇幅。

接下来给出以train为学习样本，test为测试样例的结果。**配置epoch12次，损失函数按照“意外发现”，学习率为0.1** 。因为上述实验中我们得知，高斯分布和均匀分布各有千秋，高斯分布有概率出现更高的准确率，但是不稳定，而均匀分布能稳定出现较高准确率，因此以下将尝试对两种随机取数进行对比。

| 概率函数 |         正确数          |              正确率               |
| :------: | :---------------------: | :-------------------------------: |
| 均匀分布 | 371\|371\|371\|371\|371 | 0.371\|0.371\|0.371\|0.371\|0.371 |
| 高斯分布 | 291\|160\|361\|314\|218 | 0.291\|0.160\|0.194\|0.314\|0.218 |

- ##### 可以看到高斯分布并不像先前一样，得出更高准确率的值。

- ##### 原因分析：也许是因为学习样例train过少的缘故。

- 除此之外也可以出，学习样例较少的train的最高准确率是低于学习样例多的test，因此对于神经网络而言，学习样本对于准确率也有关系。

#### 实验结果续

​	应实验要求将损失和学习率对准确率影响的可视化。由先前分析，我们已经得到使准确率最高的实验配置，以下将展示相关实验截图：

- #### 使用test来学习，train来测试

  设有12次epoch，结果截图：

  - #### 以下学习率为0.1

    <img src="img/r6.png">

    - 可以看到学习样例的准确率最高可以达到0.845，而之后逐渐下降。

    **以下为损失图像**

    <img src="img/r7.png">

    - 损失值随着epoch次数增加逐渐稳定，可以看到损失值大致按照上图中最后一次epoch值分布，**大致有0.59概率为0，即无误差(结果正确)**，而剩下损失值大致分布在-2e-17左右。

  - #### 以下学习率为0.007

    <img src="img/r0.007.png">

    - 图像以及数据都类似与0.01学习率

  - #### 以下学习率为0.01

    <img src="img/r1-0.01.png">

    - 学习样本准确率随着epoch次数增加而增加，最高到0.89
    - 但测试的准确率只有0.3198

    <img src="img/r2-0.01.png">

    - 损失函数值更加松散

  - #### 以下学习率为0.05

    <img src="img/r1-0.05.png">

    - 跟**0.1学习率有学习样本准确率先升后降得趋势，且最高可到达0.981**
    - 但准确率还是不如0.1学习率

    <img src="img/r2-0.05.png">

    - 损失值中间部分散乱，两遍较为平稳，但是不0的居多，所以准确率低

  - #### 以下学习率为0.15

    <img src="img/r1-0.15.png">

    - 学习效率的准确率先升后降，最高位0.67，但测试的正确率意外的高

    <img src="img/r2-0.15.png">

    - 类似与**0.1学习率，而准确率也十分接近，学习率略低的原因是损失值在-1e-17的点更多，代表误差出现概率更大**

  - #### 以下学习率为0.115

    <img src="img/r1-0.115.png">

    - 结果与0.05学习率类似

    <img src="img/r2-0.115.png">

    - 结果与0.05学习率类似，便不再赘述

  - #### 以下学习率为0.105

    <img src="img/r0.105.png">

    - 可以看到与学习率0.1类似，但是准确率更优一点点。

  - #### 以下学习率为0.1225

    <img src="img/r0.1225.png">

    - 与0.05学习率类似

  - #### 以下学习率为0.125

    <img src="img/r1-0.125.png">

    - 0.1学习率类似

    <img src="img/r2-0.125.png">

    - 0.1学习率类似

  - #### 以下学习率为0.2

    <img src="img/r1-0.2.png">

    - 各种准确率都很低，可以认为多次epoch早就达到收敛，无意义

    <img src="img/r2-0.2.png">

    - 损失函数图像非常稀疏，代表误差值非常大。

  - ### 学习率与准确率的关系图

    <img src="img/rf.png">

  由此可知学习率太大太小都不能达到最佳准确率，**要最合适的。**

- #### 使用train来学习，test来测试

  - #### 学习率为0.007

    <img src="img/ro.png">

  - #### 学习率为0.15

    <img src="img/ro6.png">

  - #### 学习率为0.1225

    <img src="img/ro8.png">

  - #### 学习率为0.125

    <img src="img/ro9.png">

  - #### 学习率为0.2

    <img src="img/ro10.png">

  - #### 除上述特地提及学习率，除了学习样本的准确率不同和损失不同，其他学习率的测试准确度都与0.007一致，图像类似，便不再赘述。

  - #### 学习率与准确率的关系图

    <img src="img/rf2.png">

    **由上述结果可知，也许是学习样例太少，学习率对准确率影响不大，且几乎为0.362，少部分更优。**

#### **结论**

- 经过上述分析，以及各种调参及实验可以看出，影响神经网络准确率的因素非常多，这也是神经网络结构复杂的特性来由之一，而本次实验也只是尽可能地找出个人认为可能相关的因素进行改动与分析。
- 先前经过学习群的讨论，确信了使用train作为学习样例还是过少，因此采用test来作为学习样例，而这也说明了对于神经网络，学习样例越多，其预测结果的准确率也可能越高。
- 经过长时间的各种调试验证，结果发现各种调参还不如**“意外发现”**的结果好，而这是与权重w改动和损失函数相关的内容，**说明权重w的更改策略对于实验预测结果的准确性有很大的关系，因此选取合适的权重更改策略是提升实验结果准确率的关键。**
- 经过各种epoch数目的测试，发现**学习样本的准确率是程正态分布的样式，过低或过高的epoch都会使实验准确率降低，因此选取合适的epoch次数很重要。**
- 经过阅读《Python神经网络编程高清版》这一文献可以发觉，损失函数是可以化简为简单易懂的形式，由此得到更简单的代码实现。**所以本次实验报告可能从感官上淡化了损失函数的描述，这是因为自己已经按照文献简化了相关计算步骤，但事实上，损失函数是最重要的。**

综上，经过各种实验结果得出结论，实验学习样例越多越好。由**实验结果续**可知学习率设置应按照对应的实验数据，最合适的最佳，过大过小都不好。

#### 本次实验使用test来学习，train来测试，最佳结果为正确数96，准确率为0.38866396761133604,学习率为0.105；使用train来学习，最佳结果为正确数371，准确率为0.371，学习率为0.1，因为学习样本数少，所以学习率的影响小，准确率更稳定，在0.362左右。

## 四.参考资料
>  [[机器学习-回归问题(Regression) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/127972563)](https://blog.csdn.net/numberer/article/details/79996753)

>  [(69条消息) 神经网络——最易懂最清晰的一篇文章_illikang的博客-CSDN博客_神经网络](https://blog.csdn.net/illikang/article/details/82019945)

《Python神经网络编程高清版》[英]塔里克·拉希德	P89~107 1.14~1.15节